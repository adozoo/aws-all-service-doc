## AWS Glue执行 ETL 操作

=== AWS Glue

> - AWS Glue 是一种**完全托管的数据目录**和 **ETL（提取、转换和加载）服务**，**可简化和自动化数据发现**、**转换和作业调度中难度较大且耗时的任务**。
> - AWS Glue 使用能**识别常用数据格式**和**数据类型的预构建分类器(classifiers)抓取数据源**并**构建数据目录**，包括**``CSV``、``Apache``、``Parquet``、``JSON`` **等。

=== 架构图

image::/图片/97图片/架构图.png[架构图]

== 实验步骤

=== 创建S3存储桶

> - 请确保您位于**美国东部（弗吉尼亚北部）**us-east-1 区域。
> - 顶部菜单**导航到 S3**

image::/图片/09图片/导航到S3.png[导航到S3]

> - 在 S3 页面上，单击**``创建存储桶``**并**填写存储桶详细信息**。
> - 桶名称：输入**``mys3gluetest``**
> * 注意： S3 存储桶名称是**全局唯一**的，请**选择一个可用的名称**。
> - AWS 区域：选择**美国东部（弗吉尼亚北部）美国东部-1**
> - 对于对象所有权：选择**ACL 已启用**
> - 对于此存储桶的**“阻止公有访问”**设置部分
> * **取消选中**"阻止所有公共访问"选项，然后**选中确认**选项。
> - 将**其他设置保留**为默认值。
> - **创建存储桶按钮**
> - S3 **存储桶已创建**。

---

=== 将文件上传到 S3 存储桶并公开

> - **单击**存储桶名称**``mys3gluetest``**。
> - 在对象中，您可以看到**以下消息**
> * 此存储桶中**没有任何对象**。

image::/图片/41图片/没有对象.png[没有对象]

> - 您可以从本地计算机上传**本实验所需文件**

==== **``本实验所需文件``**位于此仓库**附件目录**

> - 将文件**上传到我们的 S3 存储桶**
> * 点击**上传**按钮。
> * 点击**添加文件**按钮。
> * 浏览您的本地文件**并选择它**
> * 单击上传按钮**上传**。
> - 您可以从屏幕顶部的传输面板中**查看上传进度**。
> - 上传文件后，它将**显示在存储桶中**。

image::/图片/97图片/显示在存储桶中.png[显示在存储桶中]

> - 单击**``权限``**选项卡，然后**配置以下内容**
> * 向下滚动到**存储桶策略**，单击右侧的**编辑**按钮。
> * 此时将显示一个空白的**存储桶策略编辑器**。
> * 将存储桶的 ARN 复制到**剪贴板**。
> * 例如**``arn:aws:s3:::mys3gluetest``**
> - 复制下方**整个策略**，将其**粘贴到存储桶策略**中，
> - 下面 JSON 中的 ``Resource`` **改为自己的存储桶ARN**

```json
{
    "Version": "2012-10-17",
    "Id": "MYBUCKETPOLICY",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::mys3gluetest/*"
        },
        {
            "Effect": "Allow",
            "Principal": {
                "AWS": "*"
            },
            "Action": "s3:PutObject",
            "Resource": "arn:aws:s3:::mys3gluetest/*"
        },
        {
            "Effect": "Allow",
            "Principal": {
                "AWS": "*"
            },
            "Action": "s3:DeleteBucket",
            "Resource": "arn:aws:s3:::mys3gluetest"
        }
    ]
}
```

> - 点击**``保存更改``**按钮**``公开S3存储桶``**。

---

=== 创建 Glue 爬虫

> - 请确保您位于**美国东部（弗吉尼亚北部）**us-east-1 区域。
> - 顶部菜单**导航到 AWS Glue**
> - 单击**"数据目录"**部分下的**"爬网程序"**。
> - 要**创建爬网程序**，请单击**添加爬网程序**该按钮。
> * 第 1 部分：**添加有关爬网程序的信息**
> ** 爬虫名称：输入**``awsCrawler``**
> ** 单击**"下一步"**按钮。
> * 第 2 部分：**指定爬网程序源类型**
> ** 爬网程序源类型：选择**``数据存储``**
> ** 对 S3 数据存储重复爬网：选择对**所有文件夹进行爬网**
> ** 单击**"下一步"**按钮。

image::/图片/97图片/爬网程序源.png[爬网程序源]

> - 第 3 部分：**添加数据存储**
> * 选择数据存储：选择**``S3``**
> * 连接：**留空**
> * 对**"我的账户中的指定路径"**中的数据**进行爬网**
> * 包含路径：粘贴**复制的 S3 URI**，即**``s3://mys3gluetest/sample_data.csv``**
> * 单击**"下一步"**按钮。
> - 第 4 部分：**添加另一个数据存储**
> * 选择：**``否``**
> * 单击**"下一步"**按钮。
> - 第 5 部分：**选择一个 IAM 角色**
> * 选择：选择**创建 IAM 角色**
> * IAM 角色：**``AWSGlueServiceRole-aws``**
> * 单击**"下一步"**按钮。
> - 第 6 部分：**为此爬网程序创建计划**
> * 频率：选择**按需运行**
> * 单击**"下一步"**按钮。
> - 第 7 部分：**配置爬网程序的输出**
> * 单击**"添加数据库"**按钮。
> * 数据库名称：输入**``demo``**
> * 将其他选项**保留为默认值**。
> * 单击**"创建"**按钮。
> * 将其他选项**保留为默认值**。
> * 单击**"下一步"**按钮。
> - 查看**所有选项**，然后单击**"完成"**按钮，最后**创建爬网程序**。
> - 现在**已创建爬网程序**。

image::/图片/97图片/已创建爬网程序.png[已创建爬网程序]

---

=== 运行爬网程序以创建表

> - 选择**爬网程序**，然后单击**运行爬网程序**按钮。
> - 爬网程序将**尝试运行**。
> - 它将运行**近一分钟**。
> - 爬网程序**运行已完成**，现在**已创建一个表**。

image::/图片/97图片/已创建一个表.png[已创建一个表]

---

=== 创建 Glue 作业

> - 单击**"ETL"**部分下的**"作业"**。
> - 单击**添加作业**按钮。
> - 第 1 部分：**配置作业属性**
> * 名称：输入**``awsJob``**
> * IAM 角色：选择**``AWSGlueServiceRole-aws``**
> * 类型： 选择**``Spark``**
> * 胶水版本：选择**``Spark 2.4、Python 3(Glue version 1.0)``**
> * 此作业运行：选择**``由 AWS Glue 生成的拟定脚本``**
> * 脚本文件名：**``awsJob``**
> * 存储脚本所在的 S3 路径：选择 s3 存储桶即**``s3：//mys3gluetest/``**
> * 临时目录：选择 s3 存储桶即**``s3：//mys3gluetest/``**
> * 高级属性：将其**保留为默认值**
> * 监控选项：将其**保留为默认值**
> * 标签（可选）：将其**保留为默认值**
> * 打开**安全配置、脚本库和作业参数（可选）**
> * 将其他选项**保留为默认值**
> * 工作人员类型：选择**"标准"**
> * 最大容量：输入 **``10``**
> * 最大并发数：输入 **``1``**
> * 作业超时（分钟）：输入**``10``**
> * 将其他选项**保留为默认值**
> * 单击**"下一步"**按钮。
> - 第 2 部分：**选择一个数据源**
> * 选择表名**``sample_data_csv``**，这是由爬网程序使用 S3 存储桶中存在的**``sample_data.csv``**创建的。
> * 单击**"下一步"**按钮。
> - 第 3 部分：选择**转换类型**
> * 选择：**更改架构**
> * 单击**"下一步"**按钮。
> - 第 4 部分：**选择数据目标**
> * 选择：**使用数据目录中的表并更新数据目标**
> * 选择表格：选择**``sample_data_csv``**
> * 单击**"下一步"**按钮。
> - 第 5 部分：**输出架构定义**
> * 将其**保留为默认值**。
> * 单击**保存作业并编辑脚本**按钮。
> - **关闭脚本编辑器提示窗口**。
> - 您将看到默认代码，**删除所有内容**，并将其**替换为以下代码**。

```py
  from datetime import datetime
  from pyspark.context import SparkContext
  import pyspark.sql.functions as f

  from awsglue.utils import getResolvedOptions
  from pyspark.context import SparkContext
  from awsglue.context import GlueContext
  from awsglue.dynamicframe import DynamicFrame
  from awsglue.job import Job

  spark_context = SparkContext.getOrCreate()
  glue_context = GlueContext(spark_context)
  session = glue_context.spark_session

  glue_db = "demo"
  glue_tbl = "sample_data_csv"

  s3_write_path = "s3://"

  dt_start = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
  print("Start time:", dt_start)

  dynamic_frame_read = glue_context.create_dynamic_frame.from_catalog(database = glue_db, table_name = glue_tbl)

  data_frame = dynamic_frame_read.toDF()

  decode_col = f.floor(data_frame["year"]/10)*10
  data_frame = data_frame.withColumn("decade", decode_col)


  data_frame_aggregated = data_frame.groupby("decade").agg(
      f.count(f.col("movie")).alias('movie_count'),
      f.mean(f.col("rating")).alias('rating_mean'),
      )

  data_frame_aggregated = data_frame_aggregated.orderBy(f.desc("movie_count"))

  data_frame_aggregated.show(10)

  data_frame_aggregated = data_frame_aggregated.repartition(1)

  dynamic_frame_write = DynamicFrame.fromDF(data_frame_aggregated, glue_context, "dynamic_frame_write")

  glue_context.write_dynamic_frame.from_options(
      frame = dynamic_frame_write,
      connection_type = "s3",
      connection_options = {
          "path": s3_write_path,
      },
      format = "csv"
  )

  dt_end = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
  print("Start time:", dt_end)
```

> - Glue脚本中**需要修改**的地方：
> * 在第 15 行，确保您的**数据库名称正确无误**
> * 在第 16 行，确保您的**Glue表名称正确**
> * 在第 18 行中，将 S3 存储桶名称**替换为此帐户中存在的存储桶名称**
> * 注意：如果您错过了替换第 18 行上的存储桶名称，则输出将**不会发送到 S3 存储桶**。
> - 通过单击**"保存"**按钮**保存代码**。
> - 若要运行作业，请单击**"运行作业"**按钮。
> - 单击右侧的 X 图标以**关闭代码编辑器**并**检查"运行"状态**。
> - 作业当前处于**"正在运行"**状态。

image::/图片/97图片/正在运行.png[正在运行]

> - Glue作业将**在 10 分钟**内**显示为成功**。

image::/图片/97图片/显示为成功.png[显示为成功]

---

=== 检查Glue作业的输出

> - 请确保您位于**美国东部（弗吉尼亚北部）**us-east-1 区域。
> - 顶部菜单**导航到 S3**
> - **打开 S3 存储桶**。
> - 在这里，您将**看到4个文件**
> - 下载名为**``run-1654425708758-part-r-00000``**的输出文件，并将此文件**另存为 CSV**。

image::/图片/97图片/另存为CSV.png[另存为CSV]

> - **选择文件**，
> - 单击**"操作"**按钮。
> - 选择以下选项**下载为**
> - 在文件末尾添加**``.csv``**，然后**使用excel打开**。

image::/图片/97图片/excel打开.png[excel打开]

---
