
## 用KinesisDataStream搭建实时数据流系统

=== Amazon Kinesis Data Streams

> - 数据流技术**使客户能够从各种来源提取**、**处理**和**分析大量的数据**。
> - Kinesis 数据流就是这样**一种可扩展**且**持久的实时数据流服务**。
> - Kinesis 数据流是**实时写入**和**读取**的**数据记录的有序序列**。
> - 数据流的**定价基于每个分区**。

=== 组件

> - 数据记录 - Kinesis Data Stream 存储的**数据单位**。
> - 数据流 - 表示一组数据记录。数据流中的**数据记录分布到分区中**。
> - 保留期 - 可从流**访问数据记录的时间长度**。Kinesis 数据流存储的记录**默认为 24 小时**，**最长可达 365 天**。
> - Kinesis 客户端库 - 确保**每个分区**都有一个记录处理器在**运行并处理分区**。
> - 生产者将**数据记录放入分区中**。
> - 消费者**从分区获取数据记录**
> - 分区 - 它在流中具**有一系列数据记录**。
> * 可以有**多个分区**。创建数据流时会**提及所需的分区数**。
> * 流的**总容量**是其**分区的容量之和**。
> * 每个分区的数据**写入速率** - 每秒 1 MB 或 1，000 条消息。
> * 每个分区的数据**读取速率** - 每秒 2 MB。
> * 分区键 - 用于按流中的**分区对数据进行分组**。
> - 流记录可以直接**发送到``S3``**，**``Redshift``**，**``ElasticSearch``**等服务，而**不是创建消费者应用程序**。

=== 架构图

image::/图片/93图片/架构图.png[架构图]

== 实验步骤

=== 创建 IAM 角色

> - 导航到**``IAM``**
> - 在**左侧菜单**中，单击**``角色``** 。单击**``创建角色``**该按钮以**创建新的 IAM 角色**。
> - 在创建角色部分，为角色选择**可信实体类型**：
> * **AWS 服务**
> * **使用案例:Lambda**

image::/图片/09图片/IAM创建角色2.png[IAM创建角色2]

> * 单击**下一步**
> - 添加权限：现在，您可以看到**策略列表**。按名称**``AmazonS3FullAccess``**搜索权限并添加。
> - 按名称**``CloudWatchFullAccess``**搜索权限并添加。
> - 按名称**``AmazonKinesisFullAccess``**搜索权限并添加。
> - 单击**下一步**
> - 角色名称：输入 **``kinesis-datastream-role``**
> - 您**已成功**按名称 kinesis-datastream-role 创建了一个 IAM 角色。
> * 注意：您可以使用**其他名称创建角色**，然后将其**附加到 Lambda 函数**

---

=== 创建 Kinesis 数据流

> - 请确保您位于**美国东部（弗吉尼亚北部）**us-east-1 区域。
> - 顶部菜单**导航到 Kinesis**
> - 在**"入门"**下，选择**"Kinesis Data Streams"**，然后单击**"创建数据流"**
> - 在**"数据流名称"**下，输入名称**``aws-data-stream``**。
> - 容量模式：选择**预置**
> - 在**预置分区**中，输入**``1``**。
> - 单击**"创建数据流"**。

image::/图片/93图片/创建数据流.png[创建数据流]

> - 创建数据流后，单击将其**打开**。
> - 单击**配置选项卡**。
> - 向下滚动到**加密**，然后单击**编辑**按钮。
> - 选中**启用服务器端加密**并**使用默认加密密钥类型**，即**``使用 AWS 托管的 CMK``**。
> - 点击**保存更改**。

image::/图片/93图片/加密.png[加密]

> - 您**已使用 AWS KMS 加密您的数据**。

---

=== 创建S3存储桶

> - 请确保您位于**美国东部（弗吉尼亚北部）**us-east-1 区域。
> - 顶部菜单**导航到 S3**

image::/图片/09图片/导航到S3.png[导航到S3]

> - 在 S3 页面上，单击**``创建存储桶``**并**填写存储桶详细信息**。
> - 桶名称：输入**``aws-datasourcetest``**
> * 注意： S3 存储桶名称是**全局唯一**的，请**选择一个可用的名称**。
> - AWS 区域：选择**美国东部（弗吉尼亚北部）美国东部-1**
> - 在存储桶**"版本控制选项**"中，选中**"启用选项**"。
> - 在**默认加密**中
> * 服务器端加密：选择**"启用"**
> * 加密密钥类型：将密钥类型选择为**``Amazon S3 托管密钥(SSE-S3)``**。
> - 将**其他设置保留**为默认值。
> - 单击**创建存储桶按钮**
> - S3 **存储桶已创建**。

image::/图片/93图片/存储桶已创建.png[存储桶已创建]

---

=== 创建生产者 Lambda 函数

==== 让我们创建 3 个 Lambda 函数。一个函数用于生产者，另外两个函数用于消费者。

> - 确保您位于**美国东部（弗吉尼亚北部）区域**。
> - 转到菜单，然后单击 **Lambda**。

image::/图片/09图片/导航到Lambda.png[导航到Lambda]

> - 单击**创建函数**该按钮。
> - 选择**``从头开始创建``**
> - 函数名称：输入 **``producer``**
> - 运行时：**Node.js 14.x**
> - 角色：在权限部分中，单击**"更改默认执行角色"**，然后单击**"使用现有角色"**。
> - 现有角色：选择**``kinesis-datastream-role``**
> - 点击**创建函数**该按钮。
> - 配置页面：在此页面上，我们需要**配置我们的 Lambda 函数**。
> - 向下滚动，可以看到**"代码源"**部分。
> - **删除 index.js 中的现有代码**。
> - 复制以下代码并将其**粘贴到您的 ``index.js`` 文件中**。

```js
  const AWS = require('aws-sdk');
  AWS.config.update({
      region: 'us-east-1'
  })
  const s3 = new AWS.S3();
  const kinesis = new AWS.Kinesis();
  exports.handler = async (event) => {
      console.log(JSON.stringify(event));
      const bucketName = event.Records[0].s3.bucket.name;
      const keyName = event.Records[0].s3.object.key;
      const params = {
          Bucket: bucketName,
          Key: keyName
      }
      await s3.getObject(params).promise().then(async (data) => {
          const dataString = data.Body.toString();
          const payload = {
              data: dataString
          }
          await sendToKinesis(payload, keyName);
      }, error => {
          console.error(error);
      })
  };
  async function sendToKinesis(payload, partitionKey) {
      const params = {
          Data: JSON.stringify(payload),
          PartitionKey: partitionKey,
          StreamName: 'aws-data-stream'
      }
      await kinesis.putRecord(params).promise().then(response => {
          console.log(response);
      }, error => {
          console.error(error);
      })
  }
```

> - 您需要将函数**``sendToKinesis``**中的**``StreamName``**更改为**正确的名称**。

image::/图片/93图片/正确的名称.png[正确的名称]

> - 通过单击**"部署"**按钮**保存函数**。

---

=== 配置 S3 存储桶事件

> - 请确保您位于**美国东部（弗吉尼亚北部）**us-east-1 区域。
> - 顶部菜单**导航到 S3**
> - 通过**单击您的存储桶**名称**进入S3存储桶**
> - 选择**属性选项卡**并**向下滚动**
> - 您将看到**事件通知（0）**选项，单击**``创建事件通知``**按钮。
> - 填写**详细信息：**
> * 名称 ： 输入**``upload-event``**
> * 前缀 - 可选：将其**留空**
> * 后缀 - 可选：输入**``.txt``**
> * 事件类型 ：选择**所有对象创建事件**
> - 目标：**选择 Lambda 函数**
> - 指定 Lambda 函数：选择从**您的 Lambda 函数中选择**
> - Lambda 函数：从下拉列表中**选择``producer``**。
> - 点击**保存更改**
> - 这意味着每当**创建对象**时，都会**触发``producer``Lambda 函数**。

image::/图片/93图片/事件通知.png[事件通知]

---

=== 创建消费者 Lambda 函数

==== 消费者1

> - 确保您位于**美国东部（弗吉尼亚北部）区域**。
> - 转到菜单，然后单击 **Lambda**。

image::/图片/09图片/导航到Lambda.png[导航到Lambda]

> - 单击**创建函数**该按钮。
> - 选择**``从头开始创建``**
> - 函数名称：输入 **``consumer1``**
> - 运行时：**Node.js 14.x**
> - 角色：在权限部分中，单击**"更改默认执行角色"**，然后单击**"使用现有角色"**。
> - 现有角色：选择**``kinesis-datastream-role``**
> - 点击**创建函数**该按钮。
> - 配置页面：在此页面上，我们需要**配置我们的 Lambda 函数**。
> - 向下滚动，可以看到**"代码源"**部分。
> - **删除 index.js 中的现有代码**。
> - 复制以下代码并将其**粘贴到您的 ``index.js`` 文件中**。

```js
  exports.handler = async (event) => {
      console.log(JSON.stringify(event));
      for (const record of event.Records) {
          const data = JSON.parse(Buffer.from(record.kinesis.data, 'base64'));
          console.log('consumer #1', data);
      }
  };
```

> - 通过单击**"部署"**按钮**保存函数**。
> - 在**同一页中**，向上滚动并点击**"添加触发器"**。
> - 在触发器配置下，搜索并选择**``Kinesis``**。
> - 在 Kinesis 流下，从列表中选择**创建的数据流**。

image::/图片/93图片/添加触发器.png[添加触发器]

> - 单击**"添加"**。
> - 注意：如果您**无法看到数据流**，请单击**"刷新"**按钮。
> - 您将**能够在 Lambda 函数**中**看到添加的触发器**。

image::/图片/93图片/消费者1.png[消费者1]

==== 消费者2

> - 单击**创建函数**该按钮。
> - 选择**``从头开始创建``**
> - 函数名称：输入 **``consumer2``**
> - 运行时：**Node.js 14.x**
> - 角色：在权限部分中，单击**"更改默认执行角色"**，然后单击**"使用现有角色"**。
> - 现有角色：选择**``kinesis-datastream-role``**
> - 点击**创建函数**该按钮。
> - 配置页面：在此页面上，我们需要**配置我们的 Lambda 函数**。
> - 向下滚动，可以看到**"代码源"**部分。
> - **删除 index.js 中的现有代码**。
> - 复制以下代码并将其**粘贴到您的 ``index.js`` 文件中**。

```js
  exports.handler = async (event) => {
      console.log(JSON.stringify(event));
      for (const record of event.Records) {
          const data = JSON.parse(Buffer.from(record.kinesis.data, 'base64'));
          console.log('consumer #2', data);
      }
  };
```

> - 通过单击**"部署"**按钮**保存函数**。
> - 在同一页中，向上滚动并点击**"添加触发器"**。
> - 在触发器配置下，搜索并选择**``Kinesis``**。
> - 在 Kinesis 流下，从列表中选择**创建的数据流**。

image::/图片/93图片/添加触发器.png[添加触发器]

> - 单击**"添加"**。
> - 注意：如果您**无法看到数据流**，请单击**"刷新"**按钮。
> - 您将能够在 Lambda 函数中**看到添加的触发器**。

image::/图片/93图片/消费者2.png[消费者2]

---

=== 创建测试文件并将其上传到 S3 存储桶

> - 打开计算机上的任何**文本编辑器**。
> - 在我的案例中，**复制并粘贴**以下数据并保存在**``test.txt``**文件中

```txt
  Hello
  This is awslabs...
  Check out our courses
  Bye bye!!!
```

> - 菜单**导航到 S3**。
> - **单击**我们之前**创建的存储桶**。
> - 在**"对象"**选项卡下，单击**"上传"**按钮。
> - 在**"文件和文件夹"**中，单击**"添加文件"**。
> - 选择之前创建的**``test.txt``**文件。
> - 选择文件后，单击**"上传"**。

---

=== 测试事件通知

> - 现在，我们已**将文件上传到 S3 存储桶**。由于我们已经**配置了事件通知**，因此应该**触发生产者 Lambda 函数**。
> - 让我们通过**检查 Lambda 函数的日志**来**测试事件通知**。
> - 确保您位于**美国东部（弗吉尼亚北部）us-east-1 区域**。
> - 菜单**导航到 CloudWatch**。
> - 在左侧面板上，单击**日志**，然后选择**``producer``日志组**。
> - 单击并**打开日志组**。
> - 在日志流中，您将找到**最新的事件**，在我们的例子中，我们**只有1个日志事件**。
> - 单击并**打开日志流**。这意味着我们的**Lambda 函数已成功触发**。
> - 单击并**展开该事件**。我们可以看到**从 S3 存储桶**触发**Lambda 函数的事件**。

image::/图片/93图片/生产者事件.png[生产者事件]

> - 然后，Lambda 函数将数据**发送到 Kinesis **并**返回一条成功消息**。

image::/图片/93图片/生产者加密.png[生产者加密]

==== 消费者1

> - 现在**返回到日志组主菜单**。
> - 点击**``consumer1``**日志组。
> - 单击并**打开日志组**。
> - 在日志流中，您将找到**最新的事件**，在我们的例子中，我们**只有1个日志事件**。
> - 单击并**打开日志流**。这意味着** Lambda 函数已执行**。
> - 我们可以从**Kinesis中看到事件对象**。

image::/图片/93图片/消费者1事件.png[消费者1事件]

> - 我们可以从事件中看到**数据被加密和编码**。
> - 我们的 Lambda 函数**已提取数据**并将其**读出**。

image::/图片/93图片/消费者1解密.png[消费者1解密]

==== 消费者2

> - 现在**返回到日志组主菜单**。
> - 点击**``consumer2``**日志组。
> - 单击并**打开日志组**。
> - 在日志流中，您将找到**最新的事件**，在我们的例子中，我们**只有1个日志事件**。
> - 单击并**打开日志流**。这意味着** Lambda 函数已执行**。
> - 我们可以从**Kinesis中看到事件对象**。

image::/图片/93图片/消费者2事件.png[消费者2事件]

> - 我们可以从事件中看到**数据被加密和编码**。
> - 我们的 Lambda 函数**已提取数据**并将其**读出**。

image::/图片/93图片/消费者2解密.png[消费者2解密]

---
